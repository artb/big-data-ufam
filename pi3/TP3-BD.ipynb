{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf = SparkConf().setAppName(\"TP3 - BD2\")\n",
    "sc = SparkSession.builder.appName(\"tp3\").config(\"spark.logConf\", \"true\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/ml-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(dataset):\n",
    "    for data in dataset:\n",
    "        if data[0] == '\\n':\n",
    "            data[0] = data[0].replace('\\n','') \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "This dataset is a collection newsgroup documents. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup_aux = list(zip(dataset.data, dataset.target.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup_aux[:] = [x for x in newsgroup_aux if (len(x[0].replace('\\n', '')) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.createDataFrame(newsgroup_aux, schema=['raw_data', 'real_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol = 'raw_data', outputCol = 'words')\n",
    "#wordsData = tokenizer.transform(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(inputCol = 'words', outputCol=\"features\", vocabSize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashLSH = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [tokenizer, vectorizer, hashLSH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_process = pipeline.fit(dataset = df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(3,[0,1,2],[12.0,...|\n",
      "|(3,[0,1],[19.0,1.0])|\n",
      "|(3,[0,1,2],[194.0...|\n",
      "|(3,[0,1,2],[5.0,1...|\n",
      "|(3,[0,1,2],[14.0,...|\n",
      "| (3,[0,1],[6.0,1.0])|\n",
      "|       (3,[0],[3.0])|\n",
      "|(3,[0,1,2],[11.0,...|\n",
      "|(3,[0,1,2],[7.0,3...|\n",
      "|(3,[0,1,2],[11.0,...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "post_process.select(\"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrutura do df:\n",
    "id: bigint\n",
    "\n",
    "raw_data: string\n",
    "\n",
    "real_value: bigint\n",
    "\n",
    "words: array<string>\n",
    "    \n",
    "features: vector\n",
    "    \n",
    "hashes: array<vector>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
